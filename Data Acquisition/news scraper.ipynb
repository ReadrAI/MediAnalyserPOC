{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept of the app\n",
    "1. Get Articles (once, weekly, daily, ...)\n",
    "\n",
    "    a. scraping & parsing\n",
    "    \n",
    "    b. API access\n",
    "\n",
    "    c. save to DB!!!\n",
    "\n",
    "2. Recognise keywords and main topic\n",
    "    \n",
    "    a. TF-IDF\n",
    "    \n",
    "    b. ???\n",
    "\n",
    "3. Group with other of the same topic\n",
    "    \n",
    "    a. Main topic matching\n",
    "    \n",
    "    b. Hierarchy (decision tree?)\n",
    "\n",
    "4. Find divergences\n",
    "\n",
    "    a. diff on keyword set\n",
    "\n",
    "    b. difference of opinion\n",
    "\n",
    "    c. difference of data sources\n",
    "\n",
    "    d. difference in \n",
    "\n",
    "5. Represent articles and divergences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import lxml\n",
    "import json\n",
    "import shutil\n",
    "import codecs\n",
    "import requests\n",
    "import psycopg2\n",
    "import sqlalchemy\n",
    "import urllib.parse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from random import random\n",
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "from selenium.webdriver import Chrome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webdriver_path = './chromedriver_v84'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getEngine():\n",
    "    global engine\n",
    "    try:\n",
    "        engine\n",
    "    except:\n",
    "        engine = getDBConnection()\n",
    "    return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = Chrome(webdriver_path)\n",
    "driver.get(\"https://www.google.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDriver():\n",
    "    global driver\n",
    "    try:\n",
    "        driver\n",
    "    except:\n",
    "        driver = Chrome(webdriver_path)\n",
    "        driver.get(\"https://www.google.com/\")\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchPage(driver, url):\n",
    "\tdriver.get(url)\n",
    "\thtml_page = driver.page_source\n",
    "\treturn html_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getArticleContent(url):\n",
    "    html_content = fetchPage(driver, url)\n",
    "    parser = NYTParser()\n",
    "    parser.feed(html_content)\n",
    "    return parser.getText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NYTParser(HTMLParser):\n",
    "    \n",
    "    output = \"\"\n",
    "    print = True\n",
    "    tag = \"\"\n",
    "    forbiddenTags = ['script', 'style', 'span', 'iframe', 'h2']\n",
    "    attrs = None\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        #print(\"Encountered a start tag:\", tag)\n",
    "        if tag in self.forbiddenTags:\n",
    "            self.print = False\n",
    "        else:\n",
    "            if tag == 'div':\n",
    "                self.output += \"\\n\"\n",
    "            self.tag = tag\n",
    "            self.attrs = attrs\n",
    "        \n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        #print(\"Encountered an end tag :\", tag)\n",
    "        if tag in self.forbiddenTags:\n",
    "            self.print = True\n",
    "        \n",
    "\n",
    "    def handle_data(self, data):\n",
    "        if self.print:\n",
    "            if (\n",
    "                len(self.attrs) == 1 and (\n",
    "                    self.attrs[0][0] == 'class' or self.attrs[0][0] == 'data-rh'\n",
    "                )\n",
    "            ) or (\n",
    "                len(self.attrs) >= 3 and self.attrs[1][0] == 'href' and self.attrs[1][1].startswith(\"https://\")\n",
    "            ):\n",
    "                self.output = self.output + data + (\"\\n\" if data.endswith(\"\\n\") else \"\")\n",
    "                self.attrs == None\n",
    "                #print(self.tag, self.attrs, data)\n",
    "            else:\n",
    "                #print(self.tag, data)\n",
    "                pass\n",
    "            \n",
    "    def getText(self):\n",
    "        text = self.output\n",
    "        x = len(text)\n",
    "        while True:\n",
    "            text = text.replace(\"\\t\", \"\").replace(\"  \", \" \").replace(\"\\n\\n\", \"\")\\\n",
    "                .replace(\"\\n \", \"\\n\").replace(\" \\n\", \"\\n\")\n",
    "            if len(text) == x:\n",
    "                return text.strip()\n",
    "            else:\n",
    "                x = len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\n",
    "\n",
    "\n",
    " \n",
    "\n",
    " \n",
    "\n",
    " \n",
    " \n",
    "\n",
    "   \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#formatted_html = format(parser.output)\n",
    "#print(len(formatted_html), len(format(text)), \"\\n\\n\", formatted_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def checkOutput(text, formatted_html):\n",
    "    for sub in text.split(\"\\n\\n\"):\n",
    "        i = formatted_html.find(sub)\n",
    "        if i < 0 or i >= len(formatted_html):\n",
    "            for subsub in sub.split(\"\\n\"):\n",
    "                j = formatted_html.find(subsub)\n",
    "                if j < 0 or j >= len(formatted_html):\n",
    "                    print(sub, \"\\n\", \"######################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "driver.get(\"https://www.nytimes.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible sources:\n",
    "\n",
    "    \"NYTimes\": \"https://www.nytimes.com/\",\n",
    "    \"BBC\": \"https://www.bbc.co.uk/\",\n",
    "    \"CNN\": \"https://edition.cnn.com/\",\n",
    "    \"FoxNews\": \"https://www.foxnews.com/\",\n",
    "    \"OAN\": \"https://www.oann.com/\"\n",
    "\n",
    "The Guardian, Yahoo news, Washington Post, Daily Mail, ...\n",
    "\n",
    "https://www.4imn.com/top200/ for more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUrlContent(source_name, params={}):\n",
    "    engine = getEngine()\n",
    "    sql = f\"\"\"select source_name, api_url, api_key from sources where source_name = '{source_name}'\"\"\"\n",
    "    source = engine.execute(sql).fetchall()[0]\n",
    "    url = source['api_url'].replace(\"API_KEY\", source['api_key'])\n",
    "    print(url)\n",
    "    params = {}\n",
    "    r = requests.get(url = url, params = params) \n",
    "    return r.json() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveData(data, name):\n",
    "    with open('../raw_data/%s.txt' % name, 'w') as outfile:\n",
    "        json.dump(data, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(name):\n",
    "    with open('../raw_data/%s.txt' % name) as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDBConnection(schema=None):\n",
    "    connectArgs = {}\n",
    "    if not schema is None:\n",
    "        connectArgs['options'] = '-csearch_path=' + schema\n",
    "    username = \"jean\"\n",
    "    password = \"\"\n",
    "    host = \"127.0.0.1\"\n",
    "    port = \"5432\"\n",
    "    database = \"media\"\n",
    "    postgresUrl = 'postgres://' + username + ':' + password + '@' + host + ':' + str(port) + '/' + database\n",
    "    return sqlalchemy.create_engine(postgresUrl, connect_args=connectArgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSourceID(name):\n",
    "    engine = getEngine()\n",
    "    sql = f\"\"\"select * from media.listing.sources where source_name = '{name}';\"\"\"\n",
    "    sources = engine.execute(sql).fetchall()\n",
    "    if len(sources) > 1:\n",
    "        print(\"Warning: multiple sources matching name\", name)\n",
    "        print(\"Possible matches\", [x['source_name'] for x in sources])\n",
    "    elif len(sources) == 0:\n",
    "        return None\n",
    "    return str(sources[0]['source_uuid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertSource(source_name, country, website_url, api_url, api_key):\n",
    "    engine = getEngine()\n",
    "    sql = f\"\"\"insert into media.listing.sources (source_name, country, website_url, api_url, api_key, addedat)\n",
    "        values ('{source_name}', '{country}', '{website_url}', '{api_url}', '{api_key}', current_timestamp);\"\"\"\n",
    "    try:\n",
    "        engine.execute(sql)\n",
    "    except (psycopg2.errors.UniqueViolation, sqlalchemy.exc.IntegrityError):\n",
    "        print(\"Source already exists:\", source_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertArticle(\n",
    "    article_url, \n",
    "    source_uuid, \n",
    "    provider_uuid, \n",
    "    title, \n",
    "    description, \n",
    "    author, \n",
    "    publishedAt, \n",
    "    updatedAt, \n",
    "    content=None):\n",
    "\n",
    "    engine = getEngine()\n",
    "    try : \n",
    "        engine.execute(f\"\"\"\n",
    "            insert into articles (\n",
    "                article_url, \n",
    "                source_uuid, \n",
    "                provider_uuid, \n",
    "                title, \n",
    "                description, \n",
    "                author, \n",
    "                publishedAt, \n",
    "                updatedAt)\n",
    "            values (\n",
    "                '{article_url}', \n",
    "                '{source_uuid}', \n",
    "                '{provider_uuid}', \n",
    "                '{title}', \n",
    "                '{description}', \n",
    "                '{author}', \n",
    "                '{publishedAt}', \n",
    "                '{updatedAt}');\n",
    "                    \"\"\")\n",
    "        if not description is None:\n",
    "            article_uuid_sql = f\"\"\"select article_uuid from articles where article_url = '{article_url}';\"\"\"\n",
    "            article_uuid = str(engine.execute(article_uuid_sql).fetchall()[0][0])\n",
    "            engine.execute(f\"\"\"insert into article_contents values ('{article_uuid}', '{description}');\"\"\")\n",
    "    except (psycopg2.errors.UniqueViolation, sqlalchemy.exc.IntegrityError):\n",
    "        print(\"Article article already exists:\", article_url)\n",
    "        # todo manage article duplicates with difference or different sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def importNYT(data):\n",
    "    source_name = \"NYTimes\"\n",
    "    if data['status'] == 'OK':\n",
    "        for i in range(data['num_results']):\n",
    "            article = data['results'][i]\n",
    "            source_uuid = getSourceID(source_name)\n",
    "            if source_uuid == None:\n",
    "                print(\"Source Not Found:\", source_name)\n",
    "            insertArticle(\n",
    "                article['url'], \n",
    "                source_uuid, \n",
    "                source_uuid,\n",
    "                article['title'],\n",
    "                article['abstract'],\n",
    "                article['byline'],\n",
    "                article['published_date'],\n",
    "                article['updated_date'], \n",
    "                article[])\n",
    "    else:\n",
    "        print(\"Data error:\", data['status'] if 'status' in data.keys() else \"JSON Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipelineNYT(loadDisk=False, fetchSource=False):\n",
    "    if fetchSource:\n",
    "        data = getUrlContent('NYTimes')\n",
    "        saveData(data, 'nyt_20200722_1000')\n",
    "    if loadDisk:\n",
    "        data = loadData('nyt_20200722_1000')\n",
    "    importNYT(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getSources():\n",
    "    engine = getEngine()\n",
    "    page = 0\n",
    "    articles = []\n",
    "    while True:\n",
    "        url = ('http://newsapi.org/v2/top-headlines?'\n",
    "        'pageSize=100&'\n",
    "        'page={page}&'\n",
    "        'language=en&'\n",
    "        'country=us&'\n",
    "        'apiKey=e30a64cfe1734e6794bdab67106590fa')\n",
    "        print(url)\n",
    "        response = requests.get(url)\n",
    "        json_response = response.json()\n",
    "        if json_response['status'] == 'ok':\n",
    "            articles.extend(json_response['articles'])\n",
    "            if len(articles) == json_response['totalResults']:\n",
    "                break\n",
    "            elif len(articles) > json_response['totalResults']:\n",
    "                print(\"More articles (%d) than possible to fetch (%d)\" % (\n",
    "                    len(articles), \n",
    "                    json_response['totalResults']))\n",
    "                break\n",
    "            page += 1\n",
    "        else:\n",
    "            print(\"Query status not ok\")\n",
    "            for k, v in json_response.items():\n",
    "                print(k, v)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipelineNYT(fetchSource=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_article_url = \"https://www.nytimes.com/2020/07/21/business/economy/coronavirus-unemployment-benefits.html\"\n",
    "test_article_uri = \"nyt://article/6fae2333-ff5a-5ac0-bcc5-34a2da53cbb3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}